{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import numbers\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "from utils.utils_fast import conv2\n",
    "import pickle\n",
    "from utils.load_data import load_data\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(y_pred, outpath=\"output/\"):\n",
    "    ids = np.arange(1, 2001, dtype=int)\n",
    "    arr = np.hstack((ids.reshape(-1, 1), y_pred.reshape(-1, 1)))\n",
    "    arr = arr.astype(int)\n",
    "    np.savetxt(\n",
    "        outpath + \"Yte.csv\",\n",
    "        arr,\n",
    "        header=\"Id,Prediction\",\n",
    "        fmt=\"%d,%d\",\n",
    "        delimiter=\",\",\n",
    "        comments=\"\",\n",
    "    )\n",
    "\n",
    "\n",
    "def reshape(images, sx=32, n_channels=3):\n",
    "    return images.reshape((-1, n_channels, sx, sx)).swapaxes(1, 2).swapaxes(2, 3)\n",
    "\n",
    "\n",
    "def imshow(img):\n",
    "    \"\"\"visualize image of given vec: 1 x 3072\"\"\"\n",
    "    if img.ndim < 3:\n",
    "        print(\"ok\")\n",
    "        img = img.reshape((32, 32, 3), order=\"F\")\n",
    "        img = img.swapaxes(0, 1)\n",
    "    for i in range(3):\n",
    "        minval = img[:, :, i].min()\n",
    "        maxval = img[:, :, i].max()\n",
    "        if minval != maxval:\n",
    "            img[:, :, i] -= minval\n",
    "            img[:, :, i] *= 1.0 / (maxval - minval)\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def distribution_y(filepath=\"data/\"):\n",
    "    y = np.genfromtxt(filepath + \"Ytr.csv\", delimiter=\",\", skip_header=1, dtype=int)[\n",
    "        :, -1\n",
    "    ]\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    print(y)\n",
    "    vmin = y.min()\n",
    "    vmax = y.max()\n",
    "    plt.hist(y, bins=np.arange(vmin - 0.5, vmax + 1.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_random_state(seed):\n",
    "    \"\"\"Turn seed into a np.random.RandomState instance\n",
    "    If seed is None, return the RandomState singleton used by np.random.\n",
    "    If seed is an int, return a new RandomState instance seeded with seed.\n",
    "    If seed is already a RandomState instance, return it.\n",
    "    Otherwise raise ValueError.\n",
    "    \"\"\"\n",
    "    if seed is None or seed is np.random:\n",
    "        return np.random.mtrand._rand\n",
    "    if isinstance(seed, (numbers.Integral, np.integer)):\n",
    "        return np.random.RandomState(seed)\n",
    "    if isinstance(seed, np.random.RandomState):\n",
    "        return seed\n",
    "    raise ValueError(\n",
    "        \"%r cannot be used to seed a numpy.random.RandomState\" \" instance\" % seed\n",
    "    )\n",
    "\n",
    "\n",
    "def extract_patches_2d(image, patch_size, step=1):\n",
    "    \"\"\"image : nx x ny x n_channels\n",
    "    output: n_patches x patch_size x patch_size x n_channels\n",
    "    \"\"\"\n",
    "    n_channels = image.shape[-1]\n",
    "    patches = extract_patches(image, (patch_size, patch_size, n_channels), step)\n",
    "    return patches.reshape(-1, patch_size, patch_size, n_channels)\n",
    "\n",
    "\n",
    "def extract_patches(arr, patch_shape, extraction_step):\n",
    "    if isinstance(extraction_step, int):\n",
    "        extraction_step = tuple([extraction_step] * arr.ndim)\n",
    "    patch_strides = arr.strides\n",
    "    slices = tuple(slice(None, None, st) for st in extraction_step)\n",
    "    # print(slices, type(arr), arr)\n",
    "    # slices\n",
    "    indexing_strides = arr[slices].strides\n",
    "\n",
    "    patch_indices_shape = (\n",
    "        (np.array(arr.shape) - np.array(patch_shape)) // np.array(extraction_step)\n",
    "    ) + 1\n",
    "\n",
    "    shape = tuple(list(patch_indices_shape) + list(patch_shape))\n",
    "    strides = tuple(list(indexing_strides) + list(patch_strides))\n",
    "\n",
    "    patches = as_strided(arr, shape=shape, strides=strides)\n",
    "    return patches\n",
    "\n",
    "\n",
    "def centering_patch(patch):\n",
    "    return patch - np.mean(patch, axis=(1, 2), keepdims=1)\n",
    "\n",
    "\n",
    "def shape_patch(patch, threshold=1e-6):\n",
    "    \"\"\"patch: n_patches x size x size x n_channels\n",
    "    output: n_patches x (size^2-1)n_channels\n",
    "    \"\"\"\n",
    "    n_channels = patch.shape[-1]\n",
    "    patch = patch.reshape((patch.shape[0], -1, patch.shape[-1]))\n",
    "    center = int(patch.shape[1] // 2)\n",
    "    patch = patch - patch[:, [center], :]\n",
    "    patch = np.delete(patch, center, 1)  # remove center\n",
    "    patch = patch.reshape((patch.shape[0], -1))\n",
    "    # rho = np.linalg.norm(patch, axis=1)\n",
    "    # rho[rho<threshold] = 0\n",
    "    # # binarize the difference\n",
    "    # cond = patch > 0\n",
    "    # non_cond = patch <= 0\n",
    "    # patch[cond] = 1.0\n",
    "    # patch[non_cond] = 0.0\n",
    "    return patch\n",
    "\n",
    "\n",
    "def normalize_row(X, norm=\"l2\", threshold=1e-5, return_norm=False):\n",
    "    if norm == \"l2\":\n",
    "        norms = np.linalg.norm(X, axis=1)\n",
    "        norms[norms < threshold] = threshold\n",
    "        X /= norms[:, np.newaxis]\n",
    "    if return_norm:\n",
    "        return X, norms\n",
    "    return X\n",
    "\n",
    "\n",
    "def kmeans(X, k, max_iter=1000, init=None):\n",
    "    \"\"\"normal kmeans\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    if init is None:\n",
    "        perm = np.random.choice(np.arange(n_samples), k)\n",
    "        centroids = X[perm]\n",
    "\n",
    "    norm2 = np.linalg.norm(X, axis=1, keepdims=1) ** 2\n",
    "    prev_obj = np.inf\n",
    "\n",
    "    for n_iter in range(max_iter):\n",
    "        dist2 = (\n",
    "            np.linalg.norm(centroids, axis=1) ** 2\n",
    "            - 2.0 * np.dot(X, centroids.T)\n",
    "            + norm2\n",
    "        )\n",
    "        assign = np.argmin(dist2, axis=1)\n",
    "        obj = dist2[np.unravel_index(assign, dist2.shape)].mean()\n",
    "        # print dist.shape\n",
    "        if (n_iter + 1) % 10 == 0:\n",
    "            print((\"kmeans iter %d, objective: %f\" % (n_iter + 1, obj)))\n",
    "\n",
    "        for j in range(k):\n",
    "            Xj = X[assign == j]\n",
    "            if Xj.shape[0] == 0:\n",
    "                centroids[j] = X[np.random.randint(n_samples)]\n",
    "            else:\n",
    "                centroids[j] = np.mean(Xj, axis=0)\n",
    "\n",
    "        if np.abs(prev_obj - obj) / (np.abs(obj) + 1e-20) < 1e-8:\n",
    "            break\n",
    "        prev_obj = obj\n",
    "\n",
    "        # stop criteria\n",
    "    return centroids, assign\n",
    "\n",
    "\n",
    "def spherical_kmeans(X, k, max_iter=1000, init=None):\n",
    "    \"\"\"X: n x d points with unit-norm\"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "    if init is None:\n",
    "        perm = np.random.choice(np.arange(n_samples), k)\n",
    "        centroids = X[perm]\n",
    "\n",
    "    prev_obj = np.inf\n",
    "    for n_iter in range(max_iter):\n",
    "        cos_sim = np.dot(X, centroids.T)\n",
    "        assign = np.argmax(cos_sim, axis=1)\n",
    "        obj = cos_sim[np.unravel_index(assign, cos_sim.shape)].mean()\n",
    "\n",
    "        if (n_iter + 1) % 10 == 0:\n",
    "            print((\"spherical kmeans iter %d, objective: %f\" % (n_iter + 1, obj)))\n",
    "\n",
    "        for j in range(k):\n",
    "            Xj = X[assign == j]\n",
    "            if Xj.shape[0] == 0:\n",
    "                centroids[j] = X[np.random.randint(n_samples)]\n",
    "            else:\n",
    "                centroids[j] = np.sum(Xj, axis=0)\n",
    "                norm = np.linalg.norm(centroids[j])\n",
    "                centroids[j] /= norm\n",
    "        if np.abs(prev_obj - obj) / (np.abs(obj) + 1e-20) < 1e-8:\n",
    "            break\n",
    "        prev_obj = obj\n",
    "\n",
    "    return centroids, assign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_filter_1d(shape, sigma):\n",
    "    m = (shape - 1.0) / 2.0\n",
    "    filt = np.arange(-m, m + 1)\n",
    "    filt = np.exp(-np.square(filt) / (2.0 * sigma**2))\n",
    "    return filt / np.sum(filt)\n",
    "\n",
    "\n",
    "def conv2_gaussian(in1, shape, sigma=0.5):\n",
    "    g_filter = gaussian_filter_1d(shape, sigma)\n",
    "    g_filter = np.outer(g_filter, g_filter)\n",
    "    return conv2(in1, g_filter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERTYPES = [\"gradient\", \"patch\", \"shape\"]\n",
    "\n",
    "\n",
    "class CKN(object):\n",
    "    \"\"\"docstring for CKN\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        patch_sizes=[1, 2],\n",
    "        subsamplings=[2, 4],\n",
    "        map_dims={12, 200},\n",
    "        layer_type=\"gradient\",\n",
    "    ):\n",
    "        self.patch_sizes = patch_sizes\n",
    "        self.subsamplings = subsamplings\n",
    "        self.map_dims = map_dims\n",
    "        if layer_type in LAYERTYPES:\n",
    "            self.layer_type = layer_type\n",
    "        # else:\n",
    "        #     raise ValueError(\"unknown layer type\")\n",
    "        self.n_layers = len(patch_sizes)\n",
    "\n",
    "    def train(self, images):\n",
    "        \"\"\"\n",
    "        images: n x sx x sx x channels\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        psis = images\n",
    "        n = images.shape[0]\n",
    "        for i in range(self.n_layers):\n",
    "            centering = (i == 0) and (self.layer_type == \"patch\")\n",
    "            layer = CKNLayer(\n",
    "                i,\n",
    "                centering,\n",
    "                self.layer_type,\n",
    "                self.patch_sizes[i],\n",
    "                self.map_dims[i],\n",
    "                self.subsamplings[i],\n",
    "            )\n",
    "\n",
    "            layer.train(psis)\n",
    "\n",
    "            self.layers.append(layer)\n",
    "            if i < self.n_layers - 1:\n",
    "                for j in range(n):\n",
    "                    psi = layer.forward(psis[j])\n",
    "                    if j == 0:\n",
    "                        out_psis = np.zeros((n,) + psi.shape)\n",
    "                    out_psis[j] = psi\n",
    "                psis = out_psis\n",
    "\n",
    "    def forward(self, image, layer=None):\n",
    "        \"\"\"output map of the i-th layer\n",
    "        layer: int < n_layers\n",
    "        image: sx x sx x channels\n",
    "        \"\"\"\n",
    "        if layer is None:\n",
    "            layer = self.n_layers\n",
    "        psi = image\n",
    "        for i in range(layer):\n",
    "            psi = self.layers[i].forward(psi)\n",
    "        return psi\n",
    "\n",
    "    def out_maps(self, images):\n",
    "        n = images.shape[0]\n",
    "        for i in range(n):\n",
    "            output_map = self.forward(images[i])\n",
    "            output_map = output_map.flatten()\n",
    "            if i == 0:\n",
    "                res = np.zeros((n, output_map.shape[0]))\n",
    "            res[i] = output_map\n",
    "        return res\n",
    "\n",
    "\n",
    "def save(model, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "class CKNLayer(object):\n",
    "    def __init__(self, order, centering, layer_type, patch_size, map_dim, subsampling):\n",
    "        self.order = order\n",
    "        self.centering = centering\n",
    "        self.layer_type = layer_type\n",
    "        self.patch_size = patch_size\n",
    "        self.map_dim = map_dim\n",
    "        self.subsampling = subsampling\n",
    "        self.n_patches_per_image = 10\n",
    "\n",
    "    def train(self, input_maps):\n",
    "        if self.order == 0 and self.layer_type == \"gradient\":\n",
    "            # no parameter training for gradient map\n",
    "            return\n",
    "        elif self.order == 0 and self.layer_type == \"shape\":\n",
    "            n = input_maps.shape[0]\n",
    "            for i in range(n):\n",
    "                psi = input_maps[i]\n",
    "                patch = extract_patches_2d(psi, self.patch_size)\n",
    "\n",
    "                patch = shape_patch(patch)\n",
    "                if i == 0:\n",
    "                    n_patches_per_image = np.minimum(\n",
    "                        2 * self.n_patches_per_image, patch.shape[0]\n",
    "                    )\n",
    "                    X = np.zeros((n * n_patches_per_image, patch.shape[1]))\n",
    "                # discard patches with no variance\n",
    "                idx = np.any(patch != patch[:, [0]], axis=1)\n",
    "                if n_patches_per_image <= idx.sum():\n",
    "                    patch = patch[idx]\n",
    "                np.random.shuffle(patch)\n",
    "                X[i * n_patches_per_image : (i + 1) * n_patches_per_image] = patch[\n",
    "                    :n_patches_per_image\n",
    "                ]\n",
    "            np.random.shuffle(X)\n",
    "            X = normalize_row(X, norm=\"l2\")\n",
    "            Z = spherical_kmeans(X, self.map_dim)[0]\n",
    "            self.Z = Z\n",
    "            self.sigma2 = 0.25\n",
    "            lin_tran = gaussian_func(Z.dot(Z.T), self.sigma2)\n",
    "            # svd decomposition to compute lin_tran^(-0.5)\n",
    "            w, v = np.linalg.eigh(lin_tran)\n",
    "            w = (w + 1e-8) ** (-0.5)\n",
    "            lin_tran = v.dot(np.diag(w).dot(v.T))\n",
    "            self.lin_tran = lin_tran\n",
    "        else:\n",
    "            n = input_maps.shape[0]\n",
    "            for i in range(n):\n",
    "                psi = input_maps[i]\n",
    "                patch = extract_patches_2d(psi, self.patch_size)\n",
    "                if self.centering:\n",
    "                    patch = centering_patch(patch)\n",
    "                # print patch.shape\n",
    "                patch = patch.reshape((patch.shape[0], -1))\n",
    "                if i == 0:\n",
    "                    n_patches_per_image = np.minimum(\n",
    "                        2 * self.n_patches_per_image, patch.shape[0]\n",
    "                    )\n",
    "                    X = np.zeros((n * n_patches_per_image, patch.shape[1]))\n",
    "                # discard patches with no variance\n",
    "                idx = np.any(patch != patch[:, [0]], axis=1)\n",
    "                if n_patches_per_image <= idx.sum():\n",
    "                    patch = patch[idx]\n",
    "                np.random.shuffle(patch)\n",
    "                X[i * n_patches_per_image : (i + 1) * n_patches_per_image] = patch[\n",
    "                    :n_patches_per_image\n",
    "                ]\n",
    "            np.random.shuffle(X)\n",
    "            X = normalize_row(X, norm=\"l2\")\n",
    "            Z = spherical_kmeans(X, self.map_dim)[0]\n",
    "            self.Z = Z\n",
    "            self.sigma2 = 0.25\n",
    "            lin_tran = gaussian_func(Z.dot(Z.T), self.sigma2)\n",
    "            # svd decomposition to compute lin_tran^(-0.5)\n",
    "            w, v = np.linalg.eigh(lin_tran)\n",
    "            w = (w + 1e-8) ** (-0.5)\n",
    "            lin_tran = v.dot(np.diag(w).dot(v.T))\n",
    "            self.lin_tran = lin_tran\n",
    "\n",
    "    def forward(self, input_map):\n",
    "        if self.order == 0 and self.layer_type == \"gradient\":\n",
    "\n",
    "            # this is just taking gradients w.r.t. to diff axis\n",
    "            n_channels = input_map.shape[-1]\n",
    "            output_map = np.zeros(input_map.shape[:-1] + (self.map_dim * n_channels,))\n",
    "            theta = np.linspace(0, 2.0 * np.pi, self.map_dim + 1)[:-1]\n",
    "            delta_theta = 2.0 * np.pi / self.map_dim\n",
    "            sigma2 = np.square(1 - np.cos(delta_theta)) + np.square(np.sin(delta_theta))\n",
    "            self.sigma2 = sigma2\n",
    "            for k in range(n_channels):\n",
    "                input_map_k = input_map[:, :, k]\n",
    "                dx, dy = np.gradient(input_map_k)\n",
    "                rho = np.sqrt(np.square(dx) + np.square(dy))\n",
    "                idx = rho > 0\n",
    "                dx[idx] = dx[idx] / rho[idx]\n",
    "                dy[idx] = dy[idx] / rho[idx]\n",
    "                # sample theta between [0, 2*pi]\n",
    "                for i in range(self.map_dim):\n",
    "                    output_map[:, :, i + self.map_dim * k] = rho * gaussian_func(\n",
    "                        dx * np.cos(theta[i]) + dy * np.sin(theta[i]), sigma2\n",
    "                    )\n",
    "        elif self.order == 0 and self.layer_type == \"shape\":\n",
    "            # we suppose that patch_size is odd\n",
    "            n_channels = input_map.shape[-1]\n",
    "            patch = extract_patches_2d(input_map, self.patch_size)\n",
    "\n",
    "            size = input_map.shape[0] - self.patch_size + 1\n",
    "            patch = shape_patch(patch)\n",
    "            patch, rho = normalize_row(patch, norm=\"l2\", return_norm=True)\n",
    "\n",
    "            output_map = gaussian_func(patch.dot(self.Z.T), self.sigma2)\n",
    "            output_map = np.diag(rho).dot(output_map.dot(self.lin_tran))\n",
    "            output_map = output_map.reshape((size, size, -1))\n",
    "\n",
    "        else:\n",
    "            patch = extract_patches_2d(input_map, self.patch_size)\n",
    "            size = input_map.shape[0] - self.patch_size + 1\n",
    "            # centering\n",
    "            if self.centering:\n",
    "                patch = centering_patch(patch)\n",
    "\n",
    "            patch = patch.reshape((patch.shape[0], -1))\n",
    "\n",
    "            patch, rho = normalize_row(patch, norm=\"l2\", return_norm=True)\n",
    "            output_map = gaussian_func(patch.dot(self.Z.T), self.sigma2)\n",
    "            output_map = np.diag(rho).dot(output_map.dot(self.lin_tran))\n",
    "            output_map = output_map.reshape((size, size, -1))\n",
    "\n",
    "        # gaussian smoothing\n",
    "        # print(output_map.shape)\n",
    "        for i in range(output_map.shape[-1]):\n",
    "            output_map[:, :, i] = conv2_gaussian(\n",
    "                output_map[:, :, i],\n",
    "                2 * self.subsampling + 1,\n",
    "                self.subsampling / np.sqrt(2),\n",
    "            )\n",
    "        output_map = output_map[\n",
    "            int((self.subsampling - 1) // 2) :: self.subsampling,\n",
    "            int((self.subsampling - 1) // 2) :: self.subsampling,\n",
    "            :,\n",
    "        ]\n",
    "        # print(output_map.shape)\n",
    "        # raise Exception(\"Code shall stop here\")\n",
    "\n",
    "        return output_map\n",
    "\n",
    "\n",
    "def gaussian_func(x, sigma2):\n",
    "    return np.exp(1 / sigma2 * (x - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "data_path = os.path.join(current_path, \"data\")\n",
    "# data_folder =\n",
    "X, y, X_test = load_data(data_folder=data_path)\n",
    "# print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spherical kmeans iter 10, objective: 0.736805\n",
      "spherical kmeans iter 20, objective: 0.736147\n",
      "spherical kmeans iter 30, objective: 0.736058\n",
      "spherical kmeans iter 40, objective: 0.735939\n",
      "spherical kmeans iter 50, objective: 0.735849\n",
      "spherical kmeans iter 60, objective: 0.735817\n",
      "spherical kmeans iter 70, objective: 0.735804\n",
      "spherical kmeans iter 80, objective: 0.735813\n",
      "spherical kmeans iter 90, objective: 0.735761\n",
      "spherical kmeans iter 100, objective: 0.735751\n",
      "spherical kmeans iter 110, objective: 0.735755\n",
      "spherical kmeans iter 120, objective: 0.735751\n",
      "spherical kmeans iter 130, objective: 0.735738\n",
      "spherical kmeans iter 140, objective: 0.735740\n",
      "spherical kmeans iter 150, objective: 0.735740\n",
      "spherical kmeans iter 160, objective: 0.735745\n",
      "spherical kmeans iter 170, objective: 0.735737\n",
      "spherical kmeans iter 180, objective: 0.735733\n",
      "spherical kmeans iter 190, objective: 0.735737\n",
      "ckn training time: 55.74 s\n",
      "(5000, 3200)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "train = True\n",
    "save_model = False\n",
    "prediction = False\n",
    "save_svm_model = False\n",
    "\n",
    "\n",
    "# model1 = CKN([1, 2], [2, 4], [12, 200], \"gradient\")\n",
    "# model2 = CKN([1, 2], [2, 4], [12, 200], \"patch\")\n",
    "\n",
    "# model2 = CKN([2, 2], [2, 4], [100, 200], \"patch\")\n",
    "# model3 = CKN([3], [10], [200], \"patch\")\n",
    "model4 = CKN([3, 2], [2, 4], [100, 200], \"shape\")\n",
    "models = [model1]\n",
    "\n",
    "norms = []\n",
    "X_ckn_tr = []\n",
    "for i, model in enumerate(models):\n",
    "    if train:\n",
    "        tic = time.time()\n",
    "        model.train(X)\n",
    "        toc = time.time()\n",
    "        print((\"ckn training time: %.2f s\" % (toc - tic)))\n",
    "        if save_model:\n",
    "            save(model, \"model/\" + model.layer_type + \"_\" + str(model.n_layers))\n",
    "    else:\n",
    "        model = load(\"model/\" + model.layer_type + \"_\" + str(model.n_layers))\n",
    "        models[i] = model\n",
    "\n",
    "    X_ckn = model.out_maps(X)\n",
    "    X_ckn = X_ckn - X_ckn.mean(axis=1, keepdims=1)\n",
    "    norm = np.mean(np.sqrt(np.sum(np.square(X_ckn), axis=1)))\n",
    "    X_ckn = X_ckn / norm\n",
    "    norms.append(norm)\n",
    "    X_ckn_tr.append(X_ckn)\n",
    "\n",
    "X_ckn_tr = np.hstack(X_ckn_tr)\n",
    "del X\n",
    "del X_ckn\n",
    "\n",
    "print(X_ckn_tr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SVM Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]*\n",
      "optimization finished, #iter = 744\n",
      "obj = -187.234401, rho = 0.274540\n",
      "nSV = 491, nBSV = 177\n",
      "*\n",
      "optimization finished, #iter = 699\n",
      "obj = -236.683357, rho = -0.201482\n",
      "nSV = 495, nBSV = 234\n",
      "*\n",
      "optimization finished, #iter = 647\n",
      "obj = -188.136555, rho = -0.204633\n",
      "nSV = 428, nBSV = 171\n",
      "*\n",
      "optimization finished, #iter = 599\n",
      "obj = -188.693199, rho = -0.566590\n",
      "nSV = 416, nBSV = 181\n",
      "*\n",
      "optimization finished, #iter = 592\n",
      "obj = -146.974821, rho = -0.183609\n",
      "nSV = 362, nBSV = 129\n",
      "*\n",
      "optimization finished, #iter = 598\n",
      "obj = -135.544096, rho = -0.200461\n",
      "nSV = 368, nBSV = 104\n",
      "*\n",
      "optimization finished, #iter = 646\n",
      "obj = -161.012595, rho = -0.182686\n",
      "nSV = 407, nBSV = 136\n",
      "*.*\n",
      "optimization finished, #iter = 819\n",
      "obj = -303.260125, rho = -0.316938\n",
      "nSV = 603, nBSV = 326\n",
      "*\n",
      "optimization finished, #iter = 692\n",
      "obj = -177.148999, rho = 0.121934\n",
      "nSV = 449, nBSV = 167\n",
      "*\n",
      "optimization finished, #iter = 658\n",
      "obj = -140.630517, rho = -0.497333\n",
      "nSV = 406, nBSV = 110\n",
      "*\n",
      "optimization finished, #iter = 640\n",
      "obj = -138.062382, rho = -0.624769\n",
      "nSV = 406, nBSV = 117\n",
      "*\n",
      "optimization finished, #iter = 621\n",
      "obj = -117.258108, rho = -0.698868\n",
      "nSV = 353, nBSV = 84\n",
      "*\n",
      "optimization finished, #iter = 571\n",
      "obj = -113.574988, rho = -0.570288\n",
      "nSV = 348, nBSV = 88\n",
      "*\n",
      "optimization finished, #iter = 655\n",
      "obj = -153.155967, rho = -0.454654\n",
      "nSV = 419, nBSV = 126\n",
      "*\n",
      "optimization finished, #iter = 625\n",
      "obj = -129.444913, rho = -0.523885\n",
      "nSV = 393, nBSV = 97\n",
      "*\n",
      "optimization finished, #iter = 783\n",
      "obj = -212.446416, rho = -0.575896\n",
      "nSV = 521, nBSV = 207\n",
      "*\n",
      "optimization finished, #iter = 797\n",
      "obj = -225.815444, rho = -0.201348\n",
      "nSV = 553, nBSV = 210\n",
      "*.*\n",
      "optimization finished, #iter = 857\n",
      "obj = -355.686490, rho = -0.048591\n",
      "nSV = 669, nBSV = 382\n",
      "*\n",
      "optimization finished, #iter = 762\n",
      "obj = -311.181169, rho = -0.459296\n",
      "nSV = 580, nBSV = 330\n",
      "*.*\n",
      "optimization finished, #iter = 805\n",
      "obj = -330.177489, rho = -0.146626\n",
      "nSV = 619, nBSV = 349\n",
      "*\n",
      "optimization finished, #iter = 717\n",
      "obj = -242.821969, rho = -0.038020\n",
      "nSV = 495, nBSV = 233\n",
      "*.*\n",
      "optimization finished, #iter = 840\n",
      "obj = -273.910398, rho = -0.064838\n",
      "nSV = 590, nBSV = 277\n",
      "*\n",
      "optimization finished, #iter = 630\n",
      "obj = -173.287701, rho = -0.017287\n",
      "nSV = 420, nBSV = 174\n",
      "*\n",
      "optimization finished, #iter = 741\n",
      "obj = -170.232204, rho = 0.350721\n",
      "nSV = 467, nBSV = 154\n",
      "*\n",
      "optimization finished, #iter = 804\n",
      "obj = -319.467540, rho = -0.510825\n",
      "nSV = 607, nBSV = 344\n",
      "*\n",
      "optimization finished, #iter = 775\n",
      "obj = -389.339716, rho = 0.019938\n",
      "nSV = 669, nBSV = 423\n",
      "*\n",
      "optimization finished, #iter = 750\n",
      "obj = -262.893651, rho = 0.094895\n",
      "nSV = 533, nBSV = 249\n",
      "*.*\n",
      "optimization finished, #iter = 819\n",
      "obj = -275.135300, rho = 0.210109\n",
      "nSV = 578, nBSV = 275\n",
      "*\n",
      "optimization finished, #iter = 573\n",
      "obj = -165.804010, rho = 0.020941\n",
      "nSV = 387, nBSV = 156\n",
      "*\n",
      "optimization finished, #iter = 790\n",
      "obj = -200.844070, rho = 0.455744\n",
      "nSV = 504, nBSV = 187\n",
      "*\n",
      "optimization finished, #iter = 731\n",
      "obj = -267.357997, rho = 0.526446\n",
      "nSV = 525, nBSV = 269\n",
      "*\n",
      "optimization finished, #iter = 719\n",
      "obj = -234.586053, rho = 0.472355\n",
      "nSV = 493, nBSV = 235\n",
      "*\n",
      "optimization finished, #iter = 701\n",
      "obj = -273.280856, rho = 0.600222\n",
      "nSV = 535, nBSV = 292\n",
      "*\n",
      "optimization finished, #iter = 540\n",
      "obj = -147.962606, rho = 0.275147\n",
      "nSV = 353, nBSV = 133\n",
      "*\n",
      "optimization finished, #iter = 691\n",
      "obj = -166.984173, rho = 0.638391\n",
      "nSV = 444, nBSV = 155\n",
      "*\n",
      "optimization finished, #iter = 676\n",
      "obj = -225.620692, rho = 0.100077\n",
      "nSV = 465, nBSV = 216\n",
      "*\n",
      "optimization finished, #iter = 750\n",
      "obj = -286.336454, rho = 0.165403\n",
      "nSV = 575, nBSV = 304\n",
      "*\n",
      "optimization finished, #iter = 494\n",
      "obj = -118.213933, rho = -0.004067\n",
      "nSV = 318, nBSV = 102\n",
      "*\n",
      "optimization finished, #iter = 739\n",
      "obj = -169.899607, rho = 0.398953\n",
      "nSV = 465, nBSV = 146\n",
      "*\n",
      "optimization finished, #iter = 710\n",
      "obj = -187.658021, rho = 0.027118\n",
      "nSV = 453, nBSV = 167\n",
      "*\n",
      "optimization finished, #iter = 574\n",
      "obj = -128.386260, rho = 0.021449\n",
      "nSV = 348, nBSV = 110\n",
      "*\n",
      "optimization finished, #iter = 777\n",
      "obj = -176.393195, rho = 0.284887\n",
      "nSV = 481, nBSV = 154\n",
      "*\n",
      "optimization finished, #iter = 540\n",
      "obj = -126.607271, rho = 0.018754\n",
      "nSV = 347, nBSV = 108\n",
      "*\n",
      "optimization finished, #iter = 776\n",
      "obj = -197.466880, rho = 0.259887\n",
      "nSV = 496, nBSV = 178\n",
      "*\n",
      "optimization finished, #iter = 696\n",
      "obj = -196.591401, rho = 0.364850\n",
      "nSV = 468, nBSV = 183\n",
      "Total nSV = 3758\n",
      "0.617\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_tr = scaler.fit_transform(X_ckn_tr)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_tr, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "C = 1\n",
    "clf = svm.SVC(C=C, kernel=\"rbf\", verbose=True)\n",
    "# Train the classifier on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "# Make predictions on the validation data\n",
    "y_pred = clf.predict(X_val)\n",
    "# Calculate the accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.617\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'timeit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernelchallenge\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassifiers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/mva/kernel/git_folder/src/kernelchallenge/classifiers/svm.py:87\u001b[0m\n\u001b[1;32m     83\u001b[0m         X \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39msqueeze(X)\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m preds\n\u001b[0;32m---> 87\u001b[0m \u001b[38;5;28;43;01mclass\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43;01mMultiClassSVM\u001b[39;49;00m\u001b[43m:\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\n",
      "File \u001b[0;32m~/mva/kernel/git_folder/src/kernelchallenge/classifiers/svm.py:108\u001b[0m, in \u001b[0;36mMultiClassSVM\u001b[0;34m()\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m threshold\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_inference \u001b[38;5;241m=\u001b[39m (threshold \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (comp_num \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;129m@timeit\u001b[39m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y):\n\u001b[1;32m    110\u001b[0m     y_onehot \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mone_hot(y, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    111\u001b[0m     y_onehot \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m y_onehot \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'timeit' is not defined"
     ]
    }
   ],
   "source": [
    "from src.kernelchallenge.classifiers.svm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'svm.svm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m partial\n",
      "File \u001b[0;32m~/mva/kernel/git_folder/svm/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msvm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msvm\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'svm.svm'"
     ]
    }
   ],
   "source": [
    "from svm.svm import *\n",
    "import time\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252.8776957988739 0.65400004\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "gamma = 1 / X_train.shape[1]\n",
    "kernel_func = partial(rbf_kernel, gamma=gamma)\n",
    "my_svm = MultiClassSVM(num_classes=10, kernel_func=kernel_func, c=1)\n",
    "my_svm.fit(X_train, y_train)\n",
    "preds = my_svm.predict(X_val)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(elapsed_time, jnp.mean(preds == y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
